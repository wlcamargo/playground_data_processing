{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af8b85ef-76a8-4b2d-8c59-3c310f6398d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 54\u001b[0m\n\u001b[1;32m     51\u001b[0m df\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheader\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msave(filename)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Aguarda 10 segundos antes de gerar o próximo conjunto de dados\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import random\n",
    "import time\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.setAppName(\"Generate Customer Real Time\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.access.key\", \"chapolin\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.secret.key\", \"mudar@123\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.path.style.access\", True)\n",
    "conf.set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "conf.set('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider')\n",
    "conf.set(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "conf.set(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \n",
    "conf.set(\"hive.metastore.uris\", \"thrift://metastore:9083\")\n",
    "\n",
    "# Inicialização da sessão do Spark\n",
    "spark = SparkSession.builder.config(conf=conf).enableHiveSupport().getOrCreate()\n",
    "\n",
    "\n",
    "# Função para gerar dados aleatórios\n",
    "def generate_random_data():\n",
    "    id = random.randint(1, 1000)\n",
    "    firstname = f\"First_{id}\"\n",
    "    lastname = f\"Last_{id}\"\n",
    "    country = random.choice([\"Portugal\", \"USA\", \"Angola\", \"Brazil\", \"Argentina\"])\n",
    "    return (firstname, lastname, id, country)\n",
    "\n",
    "# Esquema para o DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"firstname\", StringType(), True),\n",
    "    StructField(\"lastname\", StringType(), True),\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"country\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Loop para criar arquivos de CSV com dados aleatórios a cada 10 segundos\n",
    "while True:\n",
    "    # Gera os dados aleatórios\n",
    "    data = [generate_random_data() for _ in range(100)]  # Gera 100 linhas de dados\n",
    "    \n",
    "    # Cria DataFrame com os dados gerados\n",
    "    df = spark.createDataFrame(data=data, schema=schema)\n",
    "    \n",
    "    # Salva o DataFrame como arquivo CSV no HDFS\n",
    "    timestamp = int(time.time())\n",
    "    filename = f\"s3a://landing-zone/customer_real_time/customer_{timestamp}.csv\"\n",
    "    df.write.format(\"csv\").mode(\"overwrite\").option(\"header\", \"true\").save(filename)\n",
    "    \n",
    "    # Aguarda 10 segundos antes de gerar o próximo conjunto de dados\n",
    "    time.sleep(10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
