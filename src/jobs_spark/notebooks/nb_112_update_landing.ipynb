{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86f6e6bb-d36e-4d5d-a9f7-45580a2e452c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import logging\n",
    "from configs import configs\n",
    "from functions import functions as F\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fe6ad3d-1373-4e49-83ef-b3b4683ef031",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-02 11:04:45,183 - INFO - Starting ingestions from Postgres - AdventureWorks to S3 Data Lake...\n",
      "2024-07-02 11:04:49,253 - INFO - Processing table: sales.countryregioncurrency. Source: PostgreSQL, Target: Delta Lake.\n",
      "2024-07-02 11:04:52,266 - INFO - No new data found for table sales.countryregioncurrency. Skipping ingestion.\n",
      "2024-07-02 11:04:52,284 - INFO - Processing table: humanresources.department. Source: PostgreSQL, Target: Delta Lake.\n",
      "2024-07-02 11:04:52,663 - INFO - No new data found for table humanresources.department. Skipping ingestion.\n",
      "2024-07-02 11:04:52,685 - INFO - Processing table: humanresources.employee. Source: PostgreSQL, Target: Delta Lake.\n",
      "2024-07-02 11:04:53,092 - INFO - No new data found for table humanresources.employee. Skipping ingestion.\n",
      "2024-07-02 11:04:53,119 - INFO - Processing table: sales.salesorderheader. Source: PostgreSQL, Target: Delta Lake.\n",
      "2024-07-02 11:04:53,889 - INFO - No new data found for table sales.salesorderheader. Skipping ingestion.\n",
      "2024-07-02 11:04:53,889 - INFO - Ingestions from Postgres - AdventureWorks to S3 Data Lake completed!\n"
     ]
    }
   ],
   "source": [
    "def get_credential_source():\n",
    "    \"\"\"Get the credentials for the data source.\"\"\"\n",
    "    credential_source = configs.credential_jdbc_postgres_adventureworks\n",
    "    return {\n",
    "        'url': credential_source['url'],\n",
    "        'user': credential_source['user'],\n",
    "        'password': credential_source['password'],\n",
    "        'driver': credential_source['driver']\n",
    "    }\n",
    "\n",
    "def get_data(table_input_name, spark, credential_source):\n",
    "    \"\"\"Get data from PostgreSQL table and return a DataFrame.\"\"\"\n",
    "    return spark.read \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\", credential_source['url']) \\\n",
    "        .option(\"user\", credential_source['user']) \\\n",
    "        .option(\"dbtable\", table_input_name) \\\n",
    "        .option(\"password\", credential_source['password']) \\\n",
    "        .option(\"driver\", credential_source['driver']) \\\n",
    "        .load()\n",
    "\n",
    "def compare_data(df_input_data, storage_output, spark):\n",
    "    \"\"\"Compare data from source with target and return DataFrame with new records.\"\"\"\n",
    "\n",
    "    df_output = spark.read.format(\"parquet\").load(storage_output)\n",
    "    \n",
    "    max_date_storage_output = df_output.selectExpr(\"max(modifieddate)\").collect()[0][0]\n",
    "    \n",
    "    new_records_df = df_input_data.filter(df_input_data[\"modifieddate\"] > max_date_storage_output)\n",
    "    \n",
    "    return new_records_df\n",
    "\n",
    "def process_table(table_name, df_input_data, storage_output, spark):\n",
    "    \"\"\"Process table by adding metadata and inserting new records.\"\"\"\n",
    "    logging.info(f\"Processing table: {table_name}. Source: PostgreSQL, Target: Delta Lake.\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    df_with_metadata = F.add_metadata(df_input_data)\n",
    "    \n",
    "    df_with_month_key = F.add_month_key_column(df_with_metadata, 'modifieddate')\n",
    "    \n",
    "    new_records_df = compare_data(df_with_month_key, storage_output, spark)\n",
    "    \n",
    "    if new_records_df.rdd.isEmpty():\n",
    "        logging.info(f\"No new data found for table {table_name}. Skipping ingestion.\")\n",
    "    else:\n",
    "        num_rows_written = write_data(new_records_df, storage_output, spark)\n",
    "        end_time = time.time()\n",
    "        duration = end_time - start_time\n",
    "        logging.info(f\"Table {table_name} processed in {duration:.2f} seconds. {num_rows_written} new rows written.\")\n",
    "    \n",
    "def write_data(df_with_month_key, storage_output, spark):\n",
    "    \"\"\"Save new partitioned data to target and return the number of rows written.\"\"\"\n",
    "    start_time = time.time()\n",
    "    df_with_month_key.write.format(\"parquet\").mode(\"append\").partitionBy('month_key').save(storage_output)\n",
    "    num_rows_written = df_with_month_key.count()\n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    logging.info(f\"New data processed and appended to target for table {storage_output} in {duration:.2f} seconds. {num_rows_written} rows written.\")\n",
    "    return num_rows_written\n",
    "\n",
    "def main(app_name, source_name, target_name, spark_configs):\n",
    "\n",
    "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "    logging.info(f\"Starting ingestions from {source_name} to {target_name}...\")\n",
    "\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(app_name) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", spark_configs['s3_endpoint']) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", spark_configs['s3_access_key']) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", spark_configs['s3_secret_key']) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", spark_configs['s3_path_style_access']) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", spark_configs['s3_impl']) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", spark_configs['s3_credentials_provider']) \\\n",
    "        .config(\"hive.metastore.uris\", spark_configs['hive_metastore_uris']) \\\n",
    "        .config(\"spark.sql.extensions\", spark_configs['delta_sql_extension']) \\\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", spark_configs['delta_catalog']) \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    credential_source = get_credential_source() \n",
    "    \n",
    "    for table_name in configs.tables_postgres_adventureworks.values():\n",
    "        try:\n",
    "            table_name_hdfs = F.convert_table_name(table_name)\n",
    "            df_input_data = get_data(table_name, spark, credential_source)  \n",
    "            lake_path = configs.lake_path['landing_zone_adventure_works']\n",
    "            storage_output = f\"{lake_path}{table_name_hdfs}\"\n",
    "            \n",
    "            process_table(table_name, df_input_data, storage_output, spark)\n",
    "        except Exception as e:\n",
    "\n",
    "            logging.error(f\"Error processing table {table_name}: {str(e)}\")\n",
    "\n",
    "    logging.info(f\"Ingestions from {source_name} to {target_name} completed!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app_name = 'ELT Incremental Postgres to Landing AdventureWorks'\n",
    "    source_name = 'Postgres - AdventureWorks'\n",
    "    target_name = 'S3 Data Lake'\n",
    "    spark_configs = configs.spark_configs_s3\n",
    "    \n",
    "    main(app_name, source_name, target_name, spark_configs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
